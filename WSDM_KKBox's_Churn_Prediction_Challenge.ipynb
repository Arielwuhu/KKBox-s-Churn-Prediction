{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import mysql.connector as msql\n",
    "from mysql.connector import Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the datasets are too large, do not push to github!\n",
    "# download the file on the desktop\n",
    "mypath = \"data\"\n",
    "files = [mypath+'/'+str(f) for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create data dictionary and move these 5 files into it.\n",
    "\n",
    "Output of listdir(mypath) should be:\n",
    "\n",
    "data/members_v3.csv.7z \n",
    "\n",
    "data/sample_submission_zero.csv.7z\n",
    "\n",
    "data/train.csv.7z\n",
    "\n",
    "data/transactions.csv.7z\n",
    "\n",
    "data/user_logs.csv.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/members_v3.csv.7z',\n",
       " 'data/sample_submission_zero.csv.7z',\n",
       " 'data/train.csv.7z',\n",
       " 'data/transactions.csv.7z']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary is already created\n"
     ]
    }
   ],
   "source": [
    "if \"unpacked_data\" not in listdir('./'):\n",
    "    os.makedirs(\"./unpacked_data\")\n",
    "    unpacked = False\n",
    "else:\n",
    "    print(\"Dictionary is already created\")\n",
    "    unpacked = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack data\n",
    "import py7zr\n",
    "\n",
    "if unpacked is False:\n",
    "    extract_path = \"unpacked_data\"\n",
    "    for file in tqdm.tqdm(files):\n",
    "        with py7zr.SevenZipFile(file, mode='r') as z:\n",
    "            data = z.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_data = pd.read_csv(\"unpacked_data/user_logs.csv\")\n",
    "member_dat = pd.read_csv(\"unpacked_data/members_v3.csv\")\n",
    "transactions_dat = pd.read_csv('unpacked_data/transactions.csv')\n",
    "train_dat = pd.read_csv('unpacked_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions that use to change the datatype according to columns' largest values for memory saving。\n",
    "\n",
    "For integer:\n",
    "For example, the maximum number that int8 can store is 127, and the minimum is - 128;\n",
    "if the largest values for such column is smaller than the value，\n",
    "we change the data type to int8\n",
    "Perform this for all columns include int.\n",
    "\n",
    "For float:\n",
    "We change all features to float32\n",
    "Source: https://www.kaggle.com/jeru666/did-you-think-of-these-features/notebook\n",
    "'''\n",
    "\n",
    "def change_datatype(df):\n",
    "    int_cols = list(df.select_dtypes(include=[\"int64\"]).columns)\n",
    "    for col in int_cols:\n",
    "        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):\n",
    "            df[col] = df[col].astype(np.int8)\n",
    "        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):\n",
    "            df[col] = df[col].astype(np.int16)\n",
    "        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "        else:\n",
    "            df[col] = df[col].astype(np.int64)\n",
    "            \n",
    "def change_datatype_float(df):\n",
    "    float_cols = list(df.select_dtypes(include=['float']).columns)\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>payment_method_id</th>\n",
       "      <th>payment_plan_days</th>\n",
       "      <th>plan_list_price</th>\n",
       "      <th>actual_amount_paid</th>\n",
       "      <th>is_auto_renew</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>membership_expire_date</th>\n",
       "      <th>is_cancel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YyO+tlZtAXYXoZhNr3Vg3+dfVQvrBVGO8j1mfqe4ZHc=</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>20150930</td>\n",
       "      <td>20151101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AZtu6Wl0gPojrEQYB8Q3vBSmE2wnZ3hi1FbK1rQQ0A4=</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>20150930</td>\n",
       "      <td>20151031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UkDFI97Qb6+s2LWcijVVv4rMAsORbVDT2wNXF0aVbns=</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>20150930</td>\n",
       "      <td>20160427</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M1C56ijxozNaGD0t2h68PnH2xtx5iO5iR2MVYQB6nBI=</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>20150930</td>\n",
       "      <td>20151128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yvj6zyBUaqdbUQSrKsrZ+xNDVM62knauSZJzakS9OW4=</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>20150930</td>\n",
       "      <td>20151121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  payment_method_id  \\\n",
       "0  YyO+tlZtAXYXoZhNr3Vg3+dfVQvrBVGO8j1mfqe4ZHc=                 41   \n",
       "1  AZtu6Wl0gPojrEQYB8Q3vBSmE2wnZ3hi1FbK1rQQ0A4=                 41   \n",
       "2  UkDFI97Qb6+s2LWcijVVv4rMAsORbVDT2wNXF0aVbns=                 41   \n",
       "3  M1C56ijxozNaGD0t2h68PnH2xtx5iO5iR2MVYQB6nBI=                 39   \n",
       "4  yvj6zyBUaqdbUQSrKsrZ+xNDVM62knauSZJzakS9OW4=                 39   \n",
       "\n",
       "   payment_plan_days  plan_list_price  actual_amount_paid  is_auto_renew  \\\n",
       "0                 30              129                 129              1   \n",
       "1                 30              149                 149              1   \n",
       "2                 30              129                 129              1   \n",
       "3                 30              149                 149              1   \n",
       "4                 30              149                 149              1   \n",
       "\n",
       "   transaction_date  membership_expire_date  is_cancel  \n",
       "0          20150930                20151101          0  \n",
       "1          20150930                20151031          0  \n",
       "2          20150930                20160427          0  \n",
       "3          20150930                20151128          0  \n",
       "4          20150930                20151121          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory used before reducation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory usage before reduction is 513.7384204864502 MB\n"
     ]
    }
   ],
   "source": [
    "mem = transactions_dat.memory_usage(index=True).sum()\n",
    "print(\"The memory usage before reduction is\", mem /1024**2,\"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change the datatype of transactions_dat for memory reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_datatype(transactions_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory used after reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory usage before reduction is 513.7384204864502 MB\n"
     ]
    }
   ],
   "source": [
    "mem = transactions_dat.memory_usage(index=True).sum()\n",
    "print(\"The memory usage before reduction is\", mem /1024**2,\"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering with MySQL\n",
    "\n",
    "Avoid running this section if the engineered_data.csv is already existed, engineering the first million records took 4 hours and the dataset contains more than 20 millions records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    engineered_df = pd.read_csv(\"engineered_data.csv\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while connecting to MySQL 1007 (HY000): Can't create database 'churn_prediction'; database exists\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector as msql\n",
    "from mysql.connector import Error\n",
    "\n",
    "def connect_sql(password, database = None):\n",
    "    if database == None:\n",
    "        conn = msql.connect(host='localhost', user='root',  \n",
    "                        password=password)#give ur username, password\n",
    "    else:\n",
    "        conn = msql.connect(host='localhost', user='root',  \n",
    "                        password=password, database = database)\n",
    "\n",
    "        print(\"You're connected to database: \", database)\n",
    "    return conn\n",
    "\n",
    "\n",
    "passward = \"Chen090718\"\n",
    "\n",
    "try:\n",
    "    conn = connect_sql(passward)\n",
    "    if conn.is_connected():\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"CREATE DATABASE churn_prediction\")\n",
    "        print(\"Database is created\")\n",
    "except Error as e:\n",
    "    print(\"Error while connecting to MySQL\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input transaction data into SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "msno                      object\n",
       "payment_method_id           int8\n",
       "payment_plan_days          int16\n",
       "plan_list_price            int16\n",
       "actual_amount_paid         int16\n",
       "is_auto_renew               int8\n",
       "transaction_date           int32\n",
       "membership_expire_date     int32\n",
       "is_cancel                   int8\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_dat.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(transactions_dat, train_dat, on = 'msno') # merge the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're connected to database:  churn_prediction\n",
      "Creating table....\n",
      "Table is created....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15883148it [35:26:15, 124.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# create the transactions table in sql server for later use\n",
    "\n",
    "try:\n",
    "    conn = connect_sql(passward, \"churn_prediction\")\n",
    "    if conn.is_connected():\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('DROP TABLE IF EXISTS transactions;')\n",
    "        sql = ('CREATE TABLE transactions(msno varchar(255),'+\n",
    "                'payment_method_id int,payment_plan_days int,plan_list_price int, actual_amount_paid int,is_auto_renew int,'+\n",
    "                'transaction_date int, membership_expire_date int, is_cancel int, is_churn int)')\n",
    "        cursor.execute(sql)\n",
    "        #loop through the data frame\n",
    "        for i,row in tqdm.tqdm(train_df.iterrows()):\n",
    "            #here %S means string values \n",
    "            sql = \"INSERT INTO churn_prediction.transactions VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "            cursor.execute(sql, tuple(row))\n",
    "            # the connection is not auto committed by default, so we must commit to save our changes\n",
    "            conn.commit()\n",
    "except Error as e:\n",
    "            print(\"Error while connecting to MySQL\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_avg = '''SELECT \n",
    "                msno, \n",
    "                COUNT(payment_method_id) as n_transc, \n",
    "                AVG(payment_plan_days) as mean_plan_days, \n",
    "                AVG(plan_list_price) as mean_list_price, \n",
    "                AVG(actual_amount_paid) as mean_amount_paid,\n",
    "                MAX(is_cancel)\n",
    "            FROM churn_prediction.transactions \n",
    "            GROUP BY msno'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're connected to database:  churn_prediction\n"
     ]
    }
   ],
   "source": [
    "conn = connect_sql(passward, \"churn_prediction\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(sql_avg)\n",
    "# Fetch all the records\n",
    "result = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = pd.DataFrame(result, columns=['msno', 'n_transc', 'mean_plan_days','mean_list_price', 'mean_amount_paid', \"is_cancel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_payment_mode = '''SELECT msno, payment_method_id, MAX(occurs)\n",
    "                      FROM (SELECT msno,payment_method_id, count(*) as occurs\n",
    "                            FROM churn_prediction.transactions\n",
    "                            GROUP BY msno, payment_method_id\n",
    "                      ) T1\n",
    "                      GROUP BY msno'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(sql_payment_mode)\n",
    "# Fetch all the records\n",
    "result = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_df = pd.DataFrame(result, columns=['msno', 'payment_method_id', 'occurs']).drop(\"occurs\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_auto_mode = '''SELECT msno, is_auto_renew, MAX(occurs)\n",
    "                      FROM (SELECT msno,is_auto_renew, count(*) as occurs\n",
    "                            FROM churn_prediction.transactions\n",
    "                            GROUP BY msno, is_auto_renew\n",
    "                      ) T1\n",
    "                      GROUP BY msno'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(sql_auto_mode)\n",
    "# Fetch all the records\n",
    "result = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df = pd.DataFrame(result, columns=['msno', 'payment_method_id', 'occurs']).drop(\"occurs\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_date = '''SELECT o.msno, o.transaction_date, MAX(o.payment_plan_days), MAX(o.plan_list_price), MAX(o.actual_amount_paid), o.is_churn\n",
    "              FROM churn_prediction.transactions o\n",
    "                  LEFT JOIN churn_prediction.transactions b\n",
    "                      ON o.msno = b.msno AND o.transaction_date <  b.transaction_date\n",
    "              WHERE b.transaction_date is NULL\n",
    "              GROUP BY o.msno\n",
    "             '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(sql_date)\n",
    "# Fetch all the records\n",
    "result = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = pd.DataFrame(result, columns=['msno', 'last_transaction_date', 'last_payment_days', 'last_list_price', 'last_payment', \"is_churn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_df = avg_df.merge(payment_df, on=\"msno\").merge(auto_df, on=\"msno\").merge(date_df, on=\"msno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_df.to_csv(\"engineered_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data and explore correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dat = pd.read_csv('unpacked_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.merge(train_dat, user_data, on = 'msno') \n",
    "train2 = pd.merge(train_dat, member_dat, on = 'msno') \n",
    "train3 = pd.merge(train_dat, transactions_dat, on = 'msno') \n",
    "    # system crack if merge all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2['gender'].replace({'female':'1', 'male':'0'}, inplace = True)\n",
    "train1.head()\n",
    "train2.head()\n",
    "train3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train1.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train2.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train3.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-b171cdf5c364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'is_churn'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Output: 0.9442647435443151\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train1' is not defined"
     ]
    }
   ],
   "source": [
    "X = train1.iloc[:,2:]\n",
    "y = train1.loc[:,['is_churn']]\n",
    "lr = LogisticRegression(random_state=0).fit(X, y)\n",
    "lr.predict(X)\n",
    "lr.score(X, y) # Output: 0.9442647435443151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train2.iloc[:,2:]\n",
    "y = train2.loc[:,['is_churn']]\n",
    "lr = LogisticRegression(random_state=0).fit(X, y)\n",
    "lr.predict(X)\n",
    "lr.score(X, y) # Need to debug, dataset has Null value???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train3.iloc[:,2:]\n",
    "y = train3.loc[:,['is_churn']]\n",
    "lr = LogisticRegression(random_state=0).fit(X, y)\n",
    "lr.predict(X)\n",
    "lr.score(X, y) # Output: 0.9568365792473885"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the engineered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = engineered_df.iloc[:,2:].sample(frac = 1)\n",
    "train, test = train_test_split(dat, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train['is_churn'].values\n",
    "train_X = train.drop(\"is_churn\", axis = 1).values\n",
    "test_y = test['is_churn'].values\n",
    "test_X = test.drop(\"is_churn\", axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9359877534783244"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0).fit(train_X, train_y)\n",
    "lr.predict(test_X)\n",
    "lr.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(train_X,train_y)\n",
    "Y_pred=clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9557070704527487\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "\n",
    "    keras.layers.Dense(11, activation='relu'),\n",
    "\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    \n",
    "    keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.asarray(train_X).astype('float32')\n",
    "train_y = np.asarray(train_y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "49647/49647 [==============================] - 31s 627us/step - loss: 957240634769408.0000 - accuracy: 0.9361\n",
      "Epoch 2/4\n",
      "49647/49647 [==============================] - 33s 668us/step - loss: 0.2376 - accuracy: 0.9361\n",
      "Epoch 3/4\n",
      "49647/49647 [==============================] - 34s 693us/step - loss: 0.2376 - accuracy: 0.9361\n",
      "Epoch 4/4\n",
      "49647/49647 [==============================] - 34s 690us/step - loss: 0.2376 - accuracy: 0.9361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1babb12ca90>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, epochs=4, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6206/6206 [==============================] - 4s 664us/step - loss: 0.2379 - accuracy: 0.9360\n"
     ]
    }
   ],
   "source": [
    "test_X = np.asarray(test_X).astype('float32')\n",
    "test_y = np.asarray(test_y).astype('float32')\n",
    "test_loss, test_acc = model.evaluate(test_X, test_y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9559264 , 0.04407364],\n",
       "       [0.9559264 , 0.04407364],\n",
       "       [0.9559264 , 0.04407364],\n",
       "       ...,\n",
       "       [0.9559264 , 0.04407364],\n",
       "       [0.9559264 , 0.04407364],\n",
       "       [0.9559264 , 0.04407364]], dtype=float32)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
